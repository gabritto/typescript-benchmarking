pr: none
trigger: none

resources:
  repositories:
    - repository: TypeScript
      type: github
      endpoint: Microsoft
      name: microsoft/TypeScript

  # https://stackoverflow.com/a/63276774
  pipelines:
    - pipeline: ts-main-pipeline
      source: 'TypeScript pipeline trigger'
      trigger:
        branches:
          include:
            - main

  # - name: azureSubscription
  #   value: 'TypeScript Internal Services'
  # - name: KeyVaultName
  #   value: 'jststeam-passwords'
  # - name: SecretsFilter
  #   value: 'tslab1-mseng-PAT, tsperf-azure-storage-connection-string'

parameters:
  # TODO: allow custom preset
  - name: TSPERF_PRESET
    displayName: Preset
    values:
      - full
      - fast
    default: full

variables:
  Codeql.Enabled: false
  skipComponentGovernanceDetection: true

  # This is just the version of node used to build TypeScript, run ts-perf, etc.
  # The actual benchmarked node version is controlled by ts-perf.
  NODE_VERSION: '16.x'

  REF: $[ resources.repositories['TypeScript'].ref ]
  PRETTY_REF: $[ replace(replace(replace(resources.repositories['TypeScript'].ref, '/merge', ''), 'refs/pull/', 'pr.'), 'refs/heads/', '') ]
  IS_PR: $[ startsWith(resources.repositories['TypeScript'].ref, 'refs/pull/') ]

# TODO: Generally, this pipeline should work like this:
#
# If the ref is "refs/heads/main", then we are processing a new commit to main,
# and should only run it once on a specific host, and upload to the blob store.
#
# If it's "refs/pull/<number>/merge", then we are processing a PR. HEAD will
# be the merge commit of the PR into main, where HEAD^1 will be the version
# of main that should be compared. We should run both for each benchmark.
# There's no way to trigger the pipeline this way from the UI, only via the
# pipeline run API by giving it { "resources": { "repositories": { "TypeScript": { "refName": "refs/pull/<number>/merge" } } } }
#
# There's some ambiguity here in that we may want to manually rerun old commits
# from main. Pipelines also doesn't statically provide us with the "commit"
# parameter from a manual UI run, leaving us with only the ref, which may still
# say main even if the commit is old. TODO: work out these details
# Also, we should consider self.ref (only allow when also main?)
#
# To actually run the benchmark, we'll:
# - Configuration job
#   - Run on public agents; no need to use our pool for that.
#   - Clone the benchmark repo.
#   - Generate the build configuration from the parameters, the benchmark repo,
#     and info like the current time, commit refs/
#   - Generate a matrix of builds for each host/benchmark.
# - For each over the matrix
#   - In each build, clone everything, build everything, run benchmark.
#     - Run benchmark twice if PR, comment (or whatever new infra we need)
#     - Run benchmark once on designated baseline machine if main, upload blob.

name: $(PRETTY_REF)-$(Date:yyyyMMdd).$(Rev:r)
# Hide the commit message from the run name; it'll just always say that the
# build came from the benchmarking repo's commit.
appendCommitMessageToRunName: false

jobs:
  - job: Setup
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    steps:
      - checkout: self
        path: typescript-benchmarking
        fetchTags: false
        fetchDepth: 1
        clean: true

      - checkout: TypeScript
        path: TypeScript
        fetchTags: false
        fetchDepth: 2 # For PRs, we want the merge base to compare against.
        clean: true

      - task: NodeTool@0
        inputs:
          versionSpec: $(NODE_VERSION)
        displayName: 'Install Node $(NODE_VERSION)'

      - bash: |
          mkdir artifacts
          CHECKOUT=$(Pipeline.Workspace)/TypeScript

          build_typescript() {
            NAME=$1
            OUTPUT=$2

            pushd ${CHECKOUT}

            COMMIT=$( git -C ${CHECKOUT} rev-parse HEAD )
            COMMIT_SHORT=$( git -C ${CHECKOUT} rev-parse --short HEAD )
            DATE=$( git -C ${CHECKOUT} log -1 --format=%cI )
            TIMESTAMP_DIR=$( date -d ${DATE} -u +%Y/%m/%d )

            echo "##vso[task.setvariable variable=${NAME}_COMMIT;isOutput=true]${COMMIT}"
            echo "##vso[task.setvariable variable=${NAME}_COMMIT_SHORT;isOutput=true]${COMMIT_SHORT}"
            echo "##vso[task.setvariable variable=${NAME}_DATE;isOutput=true]${DATE}"
            echo "##vso[task.setvariable variable=${NAME}_TIMESTAMP_DIR;isOutput=true]${TIMESTAMP_DIR}"

            npm ci
            npm run clean
            npm run build:compiler
            mv built/local ${OUTPUT}

            popd
          }

          if [[ $(REF) == refs/pull/* ]]; then
            build_typescript PR $(Build.SourcesDirectory)/artifacts/pr
            git -C ${CHECKOUT} switch --detach HEAD^1
          fi
          build_typescript MAIN $(Build.SourcesDirectory)/artifacts/main
        displayName: Build TypeScript

      # TODO: should this be a build cache so that it's not preserved?
      - publish: artifacts
        artifact: BuiltTypeScript
        displayName: Publish built TypeScript

      - bash: |
          # TODO: pass in custom preset parameters
          cd $(Pipeline.Workspace)/typescript-benchmarking
          node ./scripts/generateMatrix.mjs ${{ parameters.TSPERF_PRESET }}
        displayName: Generate matrix

  - job: Benchmark
    dependsOn: Setup
    pool:
      name: ts-perf-ddfun
    workspace:
      clean: all # Always start with a clean slate.

    # https://stackoverflow.com/a/69345058
    strategy:
      matrix: $[ dependencies.Setup.outputs['passOutput.matrix'] ]

    variables:
      MAIN_COMMIT: $[ dependencies.Setup.outputs['passOutput.MAIN_COMMIT'] ]
      MAIN_COMMIT_SHORT: $[ dependencies.Setup.outputs['passOutput.MAIN_COMMIT_SHORT'] ]
      MAIN_DATE: $[ dependencies.Setup.outputs['passOutput.MAIN_DATE'] ]
      MAIN_TIMESTAMP_DIR: $[ dependencies.Setup.outputs['passOutput.MAIN_TIMESTAMP_DIR'] ]
      PR_COMMIT: $[ dependencies.Setup.outputs['passOutput.PR_COMMIT'] ]
      PR_COMMIT_SHORT: $[ dependencies.Setup.outputs['passOutput.PR_COMMIT_SHORT'] ]
      PR_DATE: $[ dependencies.Setup.outputs['passOutput.PR_DATE'] ]
      PR_TIMESTAMP_DIR: $[ dependencies.Setup.outputs['passOutput.PR_TIMESTAMP_DIR'] ]

    steps:
      # TODO: figure out what perms are missing here
      # https://learn.microsoft.com/en-us/azure/devops/pipelines/release/key-vault-in-own-project?view=azure-devops&tabs=portal#query-and-use-secrets-in-your-pipeline
      # - task: AzureKeyVault@1
      #   inputs:
      #     # This info has been present in CI logs for years; consider these not secret.
      #     azureSubscription: $(azureSubscription)
      #     KeyVaultName: $(KeyVaultName)
      #     SecretsFilter: $(SecretsFilter)

      - bash: git config --global core.longpaths true
        displayName: Enable git long paths

      # These paths are relative to $(Pipeline.Workspace), which is the parent
      # directory of $(Build.SourcesDirectory), the default working directory.
      # TODO: pull these paths out as variables for reuse
      - checkout: self
        path: typescript-benchmarking
        fetchTags: false
        fetchDepth: 1
        clean: true

      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      - task: NodeTool@0
        inputs:
          versionSpec: $(NODE_VERSION)
        displayName: 'Install Node $(NODE_VERSION)'

      # The existence of this AzDo project/repository is not secret.
      # TODO: figure out why the delete is needed; how does the regular checkout step do a clean shallow clone?
      # Maybe this should instead use a service connection + checkout? (But, this is temporary anyway.)
      # TODO: also, this sometimes fails to clone; add a retry?
      - bash: git clone --depth=1 https://mseng:${PAT}@mseng.visualstudio.com/Typescript/_git/Typescript internal
        env:
          PAT: $(tslab1-mseng-PAT)
        displayName: Clone internal repo

      - bash: |
          cd internal/scripts/perf
          npx yarn install --frozen-lockfile --prefer-offline
          npx yarn gulp build
        displayName: Build ts-perf

      # This is provided by the agent.
      - bash: sudo pyperf system tune
        displayName: Tune system

      - bash: node internal/scripts/perf/bin/ts-perf host install --host $(TSPERF_HOST)
        displayName: Install hosts

      - bash: |
          mkdir artifacts

          run_benchmark () {
            node internal/scripts/perf/bin/ts-perf benchmark tsc \
              --cpus ${TSPERF_AGENT_BENCHMARK_CPU} \
              --iterations $(TSPERF_ITERATIONS) \
              --host $(TSPERF_HOST) \
              --scenario $(TSPERF_SCENARIO) \
              "$@"
          }

          if [[ $(REF) == refs/pull/* ]]; then
            echo Running PR benchmark
            run_benchmark \
              --tsc $(Pipeline.Workspace)/BuiltTypeScript/pr/tsc.js \
              --save artifacts/pr_$(TSPERF_HOST)_$(TSPERF_SCENARIO).tsc.benchmark \
              --date $(PR_DATE) \
              --repositoryType git \
              --repositoryUrl ${REPOSITORY_URI} \
              --repositoryBranch $(REF) \
              --repositoryCommit $(PR_COMMIT) \
              --repositoryDate $(PR_DATE)
            echo Running baseline benchmark
          fi
          run_benchmark \
            --tsc $(Pipeline.Workspace)/BuiltTypeScript/main/tsc.js \
            --save artifacts/main_$(TSPERF_HOST)_$(TSPERF_SCENARIO).tsc.benchmark \
            --date $(MAIN_DATE) \
            --repositoryType git \
            --repositoryUrl ${REPOSITORY_URI} \
            --repositoryBranch $(REF) \
            --repositoryCommit $(MAIN_COMMIT) \
            --repositoryDate $(MAIN_DATE)
        displayName: Run tsc benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_KIND'], 'tsc'))

      - publish: artifacts
        artifact: Benchmark_$(TSPERF_HOST)_$(TSPERF_SCENARIO)
        displayName: Publish benchmarks

      # TODO: other benchmark kinds

  - job: ProcessResults
    dependsOn: Benchmark
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    steps:
      - checkout: none

      - download: current
        patterns: '**/*.benchmark'
        displayName: Download benchmarks

      - bash: |
          mkdir artifacts
          mv $(Pipeline.Workspace)/**/*.benchmark artifacts
        displayName: Move artifacts

      # TODO: We only need this because ts-perf requires it even for loading existing benchmarks
      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      - bash: git clone --depth=1 --branch=gabritto/mergeBenchmark https://mseng:${PAT}@mseng.visualstudio.com/Typescript/_git/Typescript internal
        env:
          PAT: $(tslab1-mseng-PAT)
        displayName: Clone internal repo

      - bash: |
          cd internal/scripts/perf
          npx yarn install --frozen-lockfile --prefer-offline
          npx yarn gulp build
        displayName: Build ts-perf

      - bash: |
          shopt -s globstar
          mkdir output

          if [[ $(REF) == refs/pull/* ]]; then
            node internal/scripts/perf/bin/ts-perf merge --output output/pr.tsc.benchmark artifacts/pr_*.tsc.benchmark
            node internal/scripts/perf/bin/ts-perf merge --output output/main.tsc.benchmark artifacts/main_*.tsc.benchmark

            node internal/scripts/perf/bin/ts-perf benchmark tsc \
              --tsc $(Pipeline.Workspace)/BuiltTypeScript/main/tsc.js \
              --baseline output/main.tsc.benchmark \
              --load output/pr.tsc.benchmark
          else
            node internal/scripts/perf/bin/ts-perf merge --output output/main.benchmark artifacts/*.tsc.benchmark
            # TODO: publish blob, only on ResourceTrigger or explicit parameter true
            node internal/scripts/perf/bin/ts-perf benchmark tsc \
              --tsc $(Pipeline.Workspace)/BuiltTypeScript/main/tsc.js \
              --load output/main.tsc.benchmark
          fi
        displayName: Merge benchmarks

      - publish: artifacts
        artifact: Benchmarks
        displayName: Publish benchmarks
