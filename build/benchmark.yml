# This pipeline handles all TypeScript benchmarking. Supported situations are:
#
# - Pushes to microsoft/TypeScript's main branch. This is triggered indirectly
#   via the ts-main-pipeline resource. In this situation, BuildReason will be
#   ResourceTrigger, and we will upload the results to the blob store.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/heads/main", "version": "<commit hash>" }
#       }
#     },
#     "templateParameters": {
#       "BASELINE_RUN": true
#     }
#   }
#   In this situation, we'll treat this as a backfill run on main and upload
#   the results to the blob store. This also works for refs/heads/release-*.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/pull/<number>/merge" }
#       }
#     }
#   }
#   In this situation, we'll treat this as a PR run, comparing the pull request
#   against its merge base with main. Its results will be published back to the
#   pull request as a comment. Note that this payload is not one that the UI
#   supports, but it's possible to manually trigger it via the API, in which
#   case the UI will properly render all details about the resources.

pr: none
trigger: none

resources:
  # Note that the repository identifiers are known to external callers; don't
  # change them without ensuring everything is updated.
  repositories:
    - repository: TypeScript
      type: github
      endpoint: Microsoft
      name: microsoft/TypeScript

  # https://stackoverflow.com/a/63276774
  pipelines:
    - pipeline: ts-main-pipeline
      source: 'TypeScript pipeline trigger'
      trigger:
        branches:
          include:
            - main

parameters:
  # TODO: allow custom preset
  - name: TSPERF_PRESET
    displayName: Preset
    # Note: keep this up to date with generateMatrix.mjs
    values:
      - full
      - regular
      - tsc-only
    default: full # Branch pushes use the defaults, so this is set to full.

  - name: BASELINE_RUN
    displayName: This is a baseline run and should be uploaded
    type: boolean
    default: false

  # PR trigger params
  - name: REQUESTING_USER
    displayName: User to tag when the results are ready (PR runs only)
    type: string
    default: '«anyone?»'
  - name: SOURCE_ISSUE
    displayName: PR ID in github (PR runs only)
    type: number
    default: 0
  - name: STATUS_COMMENT
    displayName: typescript-bot comment ID indicating that the run started (PR runs only)
    type: number
    default: 0

variables:
  Codeql.Enabled: false
  skipComponentGovernanceDetection: true

  azureSubscription: 'TypeScript Public CI'
  KeyVaultName: 'jststeam-passwords'

  REF: $[ resources.repositories['TypeScript'].ref ]
  PRETTY_REF: $[ replace(replace(replace(replace(variables['REF'], '/merge', ''), 'refs/pull/', 'pr.'), 'refs/heads/', ''), '/', '_') ]
  IS_PR: $[ startsWith(variables['REF'], 'refs/pull/') ]
  # True if this run should demand a baseline machine.
  # Note: this expression is the same as the one in the Benchmark job; make sure it stays in sync.
  USE_BASELINE_MACHINE: $[ or(eq(variables['Build.Reason'], 'ResourceTrigger'), ${{ parameters.BASELINE_RUN }}) ]
  # Only upload if the provided ref is explicitly main/release-* and we've run on the baseline machine.
  # This means that main pushes will automatically upload, but we can still queue builds off of main with a specific commit in the API.
  SHOULD_UPLOAD: $[ and(or(eq(variables['REF'], 'refs/heads/main'), startsWith(variables['REF'], 'refs/heads/release-')), variables['USE_BASELINE_MACHINE']) ]

name: $(PRETTY_REF)-$(Date:yyyyMMdd).$(Rev:r)
# Hide the commit message from the run name; it'll just always say that the
# build came from the benchmarking repo's commit.
appendCommitMessageToRunName: false

jobs:
  - job: Setup
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts

    steps:
      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/generateMatrix.mjs --preset ${{ parameters.TSPERF_PRESET }}
        displayName: Generate matrix
        name: generateMatrix

      - checkout: TypeScript
        path: TypeScript
        fetchTags: false
        fetchDepth: 2 # For PRs, we want the merge base to compare against.
        clean: true

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.mjs --outputDir $(ARTIFACTS_DIR)/pr
        displayName: Build PR TypeScript
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - bash: |
          set -eo pipefail
          git switch --detach HEAD^1
        displayName: Switch to merge base
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.mjs --baseline --outputDir $(ARTIFACTS_DIR)/baseline
        displayName: Build baseline TypeScript
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - publish: $(ARTIFACTS_DIR)
        artifact: BuiltTypeScript
        displayName: Publish built TypeScript

  - job: Benchmark
    dependsOn: Setup
    pool:
      name: ts-perf-ddfun
      # Note: this expression is the same as USE_BASELINE_MACHINE; make sure it stays in sync.
      ${{ if or(eq(variables['Build.Reason'], 'ResourceTrigger'), parameters.BASELINE_RUN) }}:
        demands: Agent.Name -equals ts-perf1
    workspace:
      clean: all # Always start with a clean slate.

    # https://stackoverflow.com/a/69345058
    strategy:
      matrix: $[ dependencies.Setup.outputs['generateMatrix.MATRIX'] ]

    variables:
      TSPERF_RUN_STARTUP: $[ or(eq(variables['TSPERF_JOB_KIND'], 'startup'), eq(variables['TSPERF_JOB_KIND'], 'all')) ]
      BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
      ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'tslab1-mseng-PAT'
        displayName: Get secrets

      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS)
      - template: templates/cloneAndBuildTsPerf.yml # Sets $(TSPERF_EXE)

      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      # This is provided by the agent.
      - bash: |
          set -eo pipefail
          sudo pyperf system tune
        displayName: Tune system

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs install-hosts
        displayName: Install hosts

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsc \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/pr \
            --save $(ARTIFACTS_DIR)/pr_$(TSPERF_JOB_NAME).tsc.benchmark
        displayName: Run PR tsc benchmark
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'), eq(variables['TSPERF_TSC'], 'true'))

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsc \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).tsc.benchmark
        displayName: Run baseline tsc benchmark
        condition: and(succeeded(), eq(variables['TSPERF_TSC'], 'true'))

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsserver \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/pr \
            --save $(ARTIFACTS_DIR)/pr_$(TSPERF_JOB_NAME).tsserver.benchmark
        displayName: Run PR tsserver benchmark
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'), eq(variables['TSPERF_TSSERVER'], 'true'))

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsserver \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).tsserver.benchmark
        displayName: Run baseline tsserver benchmark
        condition: and(succeeded(), eq(variables['TSPERF_TSSERVER'], 'true'))

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-startup \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/pr \
            --save $(ARTIFACTS_DIR)/pr_$(TSPERF_JOB_NAME).startup.benchmark
        displayName: Run PR startup benchmark
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'), eq(variables['TSPERF_STARTUP'], 'true'))

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-startup \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).startup.benchmark
        displayName: Run baseline startup benchmark
        condition: and(succeeded(), eq(variables['TSPERF_STARTUP'], 'true'))

      - publish: $(ARTIFACTS_DIR)
        artifact: PartialBenchmark_$(TSPERF_JOB_NAME)
        displayName: Publish benchmarks

  - job: ProcessResults
    dependsOn:
      - Setup
      - Benchmark
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      TSPERF_MERGE_TSC: $[ dependencies.Setup.outputs['generateMatrix.TSPERF_MERGE_TSC'] ]
      TSPERF_MERGE_TSSERVER: $[ dependencies.Setup.outputs['generateMatrix.TSPERF_MERGE_TSSERVER'] ]
      TSPERF_MERGE_STARTUP: $[ dependencies.Setup.outputs['generateMatrix.TSPERF_MERGE_STARTUP'] ]
      BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
      BENCHMARKS_DIR: $(Pipeline.Workspace)/benchmarks
      MERGED_DIR: $(Pipeline.Workspace)/merged

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'tslab1-mseng-PAT, tsperf-azure-storage-connection-string, typescript-bot-github-PAT-for-comments'
        displayName: Get secrets

      - download: current
        patterns: '**/*.benchmark'
        displayName: Download benchmarks

      # Azure Pipelines haphazardly drops all artifacts into $(Pipeline.Workspace) with no
      # way to customize where they're placed. To deal with this, we download the benchmarks
      # first and move them somewhere else, before doing any other steps that would download
      # further artifacts.
      - bash: |
          set -eo pipefail
          shopt -s globstar
          mkdir -p $(BENCHMARKS_DIR)
          mv $(Pipeline.Workspace)/**/*.benchmark $(BENCHMARKS_DIR)
        displayName: Move artifacts

      # We only need this because ts-perf requires it even for loading existing benchmarks
      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS)
      - template: templates/cloneAndBuildTsPerf.yml # Sets $(TSPERF_EXE)

      - bash: |
          set -eo pipefail
          mkdir -p $(MERGED_DIR)
        displayName: Create merged output directory

      # TODO: this is better than it used to be, but surely could still be less repetitive.
      - bash: |
          set -eo pipefail
          node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.tsc.benchmark $(BENCHMARKS_DIR)/baseline_*.tsc.benchmark

          if [[ $(REF) == refs/pull/* ]]; then
            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/pr.tsc.benchmark $(BENCHMARKS_DIR)/pr_*.tsc.benchmark

            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsc \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsc.benchmark \
              --load $(MERGED_DIR)/pr.tsc.benchmark

            echo "<h2>Compiler</h2>" >> $(Pipeline.Workspace)/comment.html
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsc \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsc.benchmark --baselineName baseline \
              --load $(MERGED_DIR)/pr.tsc.benchmark --benchmarkName pr \
              --format html-fragment \
              --quiet >> $(Pipeline.Workspace)/comment.html
          else
            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsc \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.tsc.benchmark
          fi
        displayName: Merge tsc benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_MERGE_TSC'], 'true'))

      - bash: |
          set -eo pipefail
          node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.tsserver.benchmark $(BENCHMARKS_DIR)/baseline_*.tsserver.benchmark

          if [[ $(REF) == refs/pull/* ]]; then
            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/pr.tsserver.benchmark $(BENCHMARKS_DIR)/pr_*.tsserver.benchmark

            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsserver \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsserver.benchmark \
              --load $(MERGED_DIR)/pr.tsserver.benchmark

            echo "<h2>tsserver</h2>" >> $(Pipeline.Workspace)/comment.html
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsserver \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsserver.benchmark --baselineName baseline \
              --load $(MERGED_DIR)/pr.tsserver.benchmark --benchmarkName pr \
              --format html-fragment \
              --quiet >> $(Pipeline.Workspace)/comment.html
          else
            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsserver \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.tsserver.benchmark
          fi
        displayName: Merge tsserver benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_MERGE_TSSERVER'], 'true'))

      - bash: |
          set -eo pipefail
          node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.startup.benchmark $(BENCHMARKS_DIR)/baseline_*.startup.benchmark

          if [[ $(REF) == refs/pull/* ]]; then
            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/pr.startup.benchmark $(BENCHMARKS_DIR)/pr_*.startup.benchmark

            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-startup \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.startup.benchmark \
              --load $(MERGED_DIR)/pr.startup.benchmark

            echo "<h2>Startup</h2>" >> $(Pipeline.Workspace)/comment.html
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-startup \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.startup.benchmark --baselineName baseline \
              --load $(MERGED_DIR)/pr.startup.benchmark --benchmarkName pr \
              --format html-fragment \
              --quiet >> $(Pipeline.Workspace)/comment.html
          else
            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-startup \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.startup.benchmark
          fi
        displayName: Merge startup benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_MERGE_STARTUP'], 'true'))

      - publish: $(MERGED_DIR)
        artifact: Benchmarks
        displayName: Publish benchmarks

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.mjs --fragment $(Pipeline.Workspace)/comment.html
        displayName: Publish PR comment
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        env:
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      # TODO: we should probably save the "latest" benchmarks in blob store, but
      # we need to be smarter and not overwrite newer benchmarks when the pipeline
      # ends up running out of order.

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsc \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --load $(MERGED_DIR)/baseline.tsc.benchmark \
            --saveBlob linux
        displayName: Upload tsc benchmark to blob store
        condition: and(succeeded(), eq(variables['TSPERF_MERGE_TSC'], 'true'), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-tsserver \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --load $(MERGED_DIR)/baseline.tsserver.benchmark \
            --saveBlob linux.server
        displayName: Upload tsserver benchmark to blob store
        condition: and(succeeded(), eq(variables['TSPERF_MERGE_TSSERVER'], 'true'), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.mjs benchmark-startup \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --load $(MERGED_DIR)/baseline.startup.benchmark \
            --saveBlob linux.startup
        displayName: Upload startup benchmark to blob store
        condition: and(succeeded(), eq(variables['TSPERF_MERGE_STARTUP'], 'true'), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

  - job: OnFailedPRRun
    dependsOn:
      - Setup
      - Benchmark
      - ProcessResults
    condition: and(failed(), eq(variables['IS_PR'], 'true'))
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    steps:
      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.mjs --failed
        displayName: Publish PR comment
        env:
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)
